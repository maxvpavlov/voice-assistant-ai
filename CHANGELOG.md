# Changelog

All notable changes to the voice assistant project.

## [Unreleased] - 2025-11-17

### Added - ReAct Inference Agent

#### New Files
- **inference-agent.py** - HTTP server with ReAct reasoning
  - LLM-powered command processing using Ollama
  - Implements ReAct (Reasoning + Acting) pattern
  - Streaming responses for low latency
  - Structured reasoning steps + final answer

- **AGENT_QUICKSTART.md** - 5-minute setup guide
  - Installation instructions
  - Example commands
  - Troubleshooting tips

- **INFERENCE_AGENT.md** - Comprehensive documentation (550+ lines)
  - Architecture overview
  - API reference
  - Smart home integration examples
  - Custom tool development guide
  - Security considerations
  - Performance optimization

#### Smart Home Tools
- `control_light(location, state)` - Control lights
- `control_temperature(temperature, unit)` - Set thermostat
- `get_weather(location)` - Get weather info
- `set_timer(duration, label)` - Create timers
- `run_shell_command(command)` - Execute system commands

#### Voice Assistant Enhancements
- New `display_agent_response()` method
  - Shows reasoning process step-by-step
  - Displays actions and observations
  - Highlights final answer
  - Shows completion status

#### Dependencies
- `flask>=3.0.0` - HTTP server framework
- `rich>=13.0.0` - Beautiful terminal output
- `ollama>=0.1.0` - LLM integration

### Changed
- **README.md** - Added inference agent section with examples
- **voice-part.py** - Enhanced to display agent reasoning
- **requirements.txt** - Added inference agent dependencies

### Architecture
```
Voice Assistant (voice-part.py)
    â†“ HTTP POST
Inference Agent (inference-agent.py)
    â†“ Tool calls
Smart Home / System Tools
```

### Example Workflow
1. User says: "hey edge"
2. User says: "turn on lights and set temperature to 72"
3. Voice â†’ Speech-to-text â†’ HTTP POST to agent
4. Agent reasons and acts:
   ```
   ğŸ’­ Thought: User wants lights + temperature
   ğŸ”§ Action: control_light(living_room, on)
   ğŸ‘ï¸  Observation: Light turned on
   ğŸ”§ Action: control_temperature(72, F)
   ğŸ‘ï¸  Observation: Temperature set to 72Â°F
   ğŸ’¬ Final Answer: "Done! Lights on, temp 72Â°F"
   ```
5. Voice assistant displays reasoning
6. Returns to wake word detection

## [1.1.0] - 2025-11-17

### Fixed - False Wake Word Detection Bug

#### Root Cause
Wake word detector's audio queue retained stale data after `stop()`, causing false detections when restarted after speech recognition.

#### Changes
- **wake_word_detector.py**
  - Added queue clearing in `stop()` method (lines 148-153)
  - Prevents stale audio from triggering false detections

- **voice-part.py**
  - Added 300ms transition delay before restarting detector (lines 428-430)
  - Ensures clean audio resource handoff between libraries

#### Impact
- âœ… Eliminates false wake word detections
- âœ… Proper orchestration: wake word â†’ speech â†’ wake word cycle
- âœ… No performance impact (300ms delay negligible)

#### Documentation
- **BUGFIX_FALSE_WAKE_WORD.md** - Full technical details

## [1.0.0] - 2025-11-17

### Added - Self-Contained Release

#### Major Change
Made `release/` directory fully self-contained and independently deployable.

#### Before (Coupled to Root)
```
voice-part.py
â”œâ”€ Used: release/venv/         âœ…
â”œâ”€ Used: release/src/           âœ…
â””â”€ subprocess calls:
   â”œâ”€ cd .. && ../venv/         âŒ Root venv
   â”œâ”€ ../edge-wake-word         âŒ Root script
   â”œâ”€ ../train-full-model.py    âŒ Root script
   â”œâ”€ ../training_data/         âŒ Root folder
   â””â”€ ../trained_models/        âŒ Root folder
```

#### After (Self-Contained)
```
voice-part.py
â”œâ”€ Uses: release/venv/          âœ…
â”œâ”€ Uses: release/src/           âœ…
â”œâ”€ Uses: release/train-full-model.py  âœ…
â”œâ”€ Uses: release/training_data/       âœ…
â””â”€ Uses: release/trained_models/      âœ…
```

#### Changes
1. **Copied Training Script**
   - `release/train-full-model.py` (from root)
   - No parent directory dependencies

2. **Updated voice-part.py**
   - Mic testing: Direct AudioRecorder (no subprocess)
   - Sample recording: AudioRecorder.record_sample()
   - Training: Local venv and script
   - Model paths: Local directories

3. **Created Data Directories**
   - `release/training_data/`
   - `release/trained_models/`

4. **Updated .gitignore**
   - Ignores venv, training data, trained models
   - Keeps bundled Vosk model

#### Benefits
- âœ… Independent deployment (just copy release/)
- âœ… Clear separation (dev/prod)
- âœ… Simplified installation
- âœ… Portable across machines
- âœ… No parent dependencies

#### Documentation
- **SELF_CONTAINED.md** - Full migration guide

### Added - Streaming Sentence Mode

#### Feature
Voice assistant now sends each sentence to inference endpoint **immediately** when detected, instead of waiting for silence and batching.

#### Old Behavior (Batched)
```
User speaks: "turn on lights and set temp to 70"
  ... wait for 3s silence ...
  Send combined: "turn on lights and set temp to 70"
```

#### New Behavior (Streaming)
```
User speaks: "turn on lights"
  â†’ Immediately send: "turn on lights"
User speaks: "and set temp to 70"
  â†’ Immediately send: "and set temp to 70"
  ... wait for 3s silence ...
  Done
```

#### Changes
- **VoskRecognizer.recognize_stream()**
  - Added `on_sentence_callback` parameter
  - Calls callback immediately when sentence detected

- **voice-part.py**
  - Callback sends each sentence via HTTP POST
  - Continues listening for 3 seconds after last speech
  - Tracks number of sentences sent

#### Benefits
- âš¡ ~5 seconds faster first response
- ğŸ¯ Progressive processing (backend can start early)
- ğŸ’¬ Natural conversation flow
- ğŸ”„ Better multi-step commands

#### Documentation
- **STREAMING_MODE.md** - Full technical details

## [0.9.0] - 2025-11-16

### Added - Initial Production Release

#### Core Features
- Wake word training with synthetic data augmentation
- Real-time wake word detection (openWakeWord)
- Speech recognition (Vosk)
- HTTP inference endpoint integration
- State persistence across sessions
- Cross-platform support (macOS, Linux, Raspberry Pi)

#### Files
- `voice-part.py` - Main unified application
- `src/voice_assistant/` - Core modules
  - `wake_word_detector.py`
  - `audio_recorder.py`
  - `model_trainer.py`
- `recognizers/vosk_recognizer.py` - Speech recognition
- `train-full-model.py` - Model training script

#### Documentation
- `README.md` - User guide
- `QUICKSTART.md` - Quick start
- `RASPBERRY_PI_SETUP.md` - Pi setup
- `PI_QUICKSTART.md` - Pi quick start
- `ARCHITECTURE.md` - Technical architecture
- `MULTILINGUAL_SUPPORT.md` - Language support
- `MULTILINGUAL_EXAMPLES.md` - Training examples

#### Bug Fixes (from development)
1. **Microphone conflict**: Pause/resume detector during speech
2. **Model path resolution**: Prioritize local models
3. **Thread deadlock**: Non-blocking stop from callback

## Summary

### Latest Version Features

âœ… **Self-contained deployment** - Copy release/ anywhere
âœ… **ReAct inference agent** - LLM-powered reasoning
âœ… **Smart home tools** - Lights, temperature, timers, weather
âœ… **Streaming sentences** - Low-latency command processing
âœ… **No false detections** - Fixed audio queue bug
âœ… **Cross-platform** - macOS, Linux, Raspberry Pi 5
âœ… **Extensible** - Easy to add custom tools
âœ… **Production-ready** - Error handling, retry logic, state persistence

### File Structure

```
release/
â”œâ”€â”€ voice-part.py              # Main voice assistant
â”œâ”€â”€ inference-agent.py         # ReAct inference agent
â”œâ”€â”€ train-full-model.py        # Training script
â”œâ”€â”€ requirements.txt           # All dependencies
â”‚
â”œâ”€â”€ src/                       # Source modules
â”‚   â””â”€â”€ voice_assistant/
â”‚       â”œâ”€â”€ wake_word_detector.py
â”‚       â”œâ”€â”€ audio_recorder.py
â”‚       â””â”€â”€ model_trainer.py
â”‚
â”œâ”€â”€ recognizers/               # Speech recognition
â”‚   â””â”€â”€ vosk_recognizer.py
â”‚
â”œâ”€â”€ models/                    # Downloaded & trained models
â”‚   â”œâ”€â”€ vosk-model-small-en-us-0.15/
â”‚   â””â”€â”€ *.onnx (user's trained models)
â”‚
â”œâ”€â”€ training_data/             # User's voice samples
â”‚   â””â”€â”€ <wake_word>/
â”‚       â””â”€â”€ positive/
â”‚
â”œâ”€â”€ trained_models/            # Training output
â”‚   â””â”€â”€ <wake_word>/
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ README.md              # Main guide
    â”œâ”€â”€ AGENT_QUICKSTART.md    # Agent setup (5 min)
    â”œâ”€â”€ INFERENCE_AGENT.md     # Agent reference (550 lines)
    â”œâ”€â”€ QUICKSTART.md          # Voice assistant setup
    â”œâ”€â”€ RASPBERRY_PI_SETUP.md  # Pi deployment
    â”œâ”€â”€ PI_QUICKSTART.md       # Pi quick start
    â”œâ”€â”€ ARCHITECTURE.md        # Technical details
    â”œâ”€â”€ STREAMING_MODE.md      # Sentence streaming
    â”œâ”€â”€ SELF_CONTAINED.md      # Self-contained mode
    â”œâ”€â”€ BUGFIX_FALSE_WAKE_WORD.md  # Bug fix details
    â”œâ”€â”€ MULTILINGUAL_SUPPORT.md    # Language support
    â””â”€â”€ MULTILINGUAL_EXAMPLES.md   # Training examples
```

### Getting Started

**Without Inference Agent**:
```bash

source venv/bin/activate
pip install -r requirements.txt
./voice-part.py
```

**With Inference Agent**:
```bash
# Terminal 1: Start inference agent
./inference-agent.py

# Terminal 2: Start voice assistant
./voice-part.py
```

See **AGENT_QUICKSTART.md** for full setup guide.
